{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652868ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyArrow >= 1.0.0 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.2.0-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\utils.py\u001b[0m in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mhave_arrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-55868802adff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.2.0-bin-hadoop2.7\\python\\pyspark\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mrequire_minimum_pandas_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mrequire_minimum_pyarrow_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SPARK_TESTING\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.2.0-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\utils.py\u001b[0m in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mraised_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhave_arrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         raise ImportError(\"PyArrow >= %s must be installed; however, \"\n\u001b[0m\u001b[0;32m     56\u001b[0m                           \"it was not found.\" % minimum_pyarrow_version) from raised_error\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminimum_pyarrow_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: PyArrow >= 1.0.0 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "\n",
    "spark1 = SparkSession.builder.appName(\"hw1\").getOrCreate()\n",
    "filePath = 'household_power_consumption.csv'\n",
    "# df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \";\").load(filePath)\n",
    "df = spark1.read.csv(filePath, inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fda7d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------+----------------+\n",
      "|Global_active_power|Global_reactive_power|Voltage|Global_intensity|\n",
      "+-------------------+---------------------+-------+----------------+\n",
      "|              4.216|                0.418| 234.84|            18.4|\n",
      "|               5.36|                0.436| 233.63|            23.0|\n",
      "|              5.374|                0.498| 233.29|            23.0|\n",
      "|              5.388|                0.502| 233.74|            23.0|\n",
      "|              3.666|                0.528| 235.68|            15.8|\n",
      "+-------------------+---------------------+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_list = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']\n",
    "# column_list = ['Global_active_power']\n",
    "df = df[column_list]\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4b89e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dtypes\n",
    "from pyspark.sql.types import DoubleType\n",
    "for column in column_list:\n",
    "    df = df.withColumn(column, df[column].cast('float'))\n",
    "# print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7a2ec724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Global_active_power  Global_reactive_power     Voltage  \\\n",
      "max                  5.388                  0.528  235.679993   \n",
      "min                  3.520                  0.418  233.289993   \n",
      "count               10.000                 10.000   10.000000   \n",
      "\n",
      "       Global_intensity  \n",
      "max                23.0  \n",
      "min                15.0  \n",
      "count              10.0  \n"
     ]
    }
   ],
   "source": [
    "# (1) Output the minimum, maximum, and count of the following columns: ‘global active power’, ‘global reactive power’, ‘voltage’, and ‘global intensity’. \n",
    "statistics_list = ['max', 'min', 'count']\n",
    "max_min_count_list = [[df.agg({column: stat}).first()[0] for stat in statistics_list] for column in column_list]\n",
    "\n",
    "max_min_count_dict = dict()\n",
    "for i in range(len(column_list)):\n",
    "    max_min_count_dict[column_list[i]] = max_min_count_list[i]\n",
    "\n",
    "max_min_count_df = pd.DataFrame(max_min_count_dict, index=statistics_list)\n",
    "print(max_min_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43bbdcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Global_active_power  Global_reactive_power     Voltage  Global_intensity\n",
      "mean             4.225600               0.496400  234.436000         18.140000\n",
      "std              0.812879               0.037957    0.821396          3.466731\n"
     ]
    }
   ],
   "source": [
    "# (2) Output the mean and standard deviation of these columns.\n",
    "statistics_list = ['mean', 'std']\n",
    "mean_std_list = [[df.agg({column: stat}).first()[0] for stat in statistics_list] for column in column_list]\n",
    "\n",
    "mean_std_dict = dict()\n",
    "for i in range(len(column_list)):\n",
    "    mean_std_dict[column_list[i]] = mean_std_list[i]\n",
    "\n",
    "mean_std_df = pd.DataFrame(mean_std_dict, index=statistics_list)\n",
    "print(mean_std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f9937bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------+----------------+--------------------+--------------------+\n",
      "|Global_active_power|Global_reactive_power|Voltage|Global_intensity|          ss_feature|              scaled|\n",
      "+-------------------+---------------------+-------+----------------+--------------------+--------------------+\n",
      "|              4.216|                0.418| 234.84|            18.4|[4.216,0.418,234....|[0.37259100642398...|\n",
      "|               5.36|                0.436| 233.63|            23.0|[5.36,0.436,233.6...|[0.98501070663811...|\n",
      "+-------------------+---------------------+-------+----------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (3) Perform min-max normalization on the columns to generate normalized output.\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.functions import split, explode, concat, concat_ws\n",
    "vector_assembler = VectorAssembler(inputCols=column_list, outputCol='ss_feature')\n",
    "temp_train = vector_assembler.transform(df)\n",
    "# temp_train.show(2)\n",
    "\n",
    "minmax_scaler = MinMaxScaler(inputCol='ss_feature', outputCol='scaled')\n",
    "train_df = minmax_scaler.fit(temp_train).transform(temp_train)\n",
    "train.show(2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
